{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-10T09:33:35.312289Z",
     "start_time": "2024-11-10T09:33:35.297921Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "\n",
    "from fs.tree import render\n",
    "from mss import mss\n",
    "import pydirectinput\n",
    "import pytesseract\n",
    "from gymnasium import Env  # Изменено на gymnasium\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ],
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:33:36.560548Z",
     "start_time": "2024-11-10T09:33:36.540904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WebGame(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Setup spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,100), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        # Capture game frames\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top': 400, 'left': 0, 'width': 1000, 'height': 500}\n",
    "        self.done_location = {'top': 205, 'left': 630, 'width': 660, 'height': 70}\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0:'space',\n",
    "            1: 'down', \n",
    "            2: 'no_op'\n",
    "        }\n",
    "        if action !=2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "\n",
    "        done, done_cap = self.get_done() \n",
    "        observation = self.get_observation()\n",
    "        reward = 1 \n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=150, y=150)\n",
    "        pydirectinput.press('space')\n",
    "        return self.get_observation()\n",
    "        \n",
    "    def render(self):\n",
    "        cv2.imshow('Game', self.current_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "         \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (100,83))\n",
    "        channel = np.reshape(resized, (1,83,100))\n",
    "        return channel\n",
    "    \n",
    "    def get_done(self):\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))\n",
    "        done_strings = ['GAME', 'GAHE']\n",
    "        done=False\n",
    "        # if np.sum(done_cap) < 44300000:\n",
    "        #     done = True\n",
    "        done = False\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done = True\n",
    "        return done, done_cap"
   ],
   "id": "982bdaa1e2451160",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:02:44.953677Z",
     "start_time": "2024-11-10T09:02:44.873952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env = WebGame()\n",
    "obs = env.get_observation()\n",
    "\n",
    "# Вывод изображения в оттенках серого\n",
    "plt.imshow(obs[0], cmap='gray')\n",
    "plt.axis('off')  # Убрать оси для чистого отображения\n",
    "plt.show()\n"
   ],
   "id": "3c50eaaa75f40787",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:37:27.534523Z",
     "start_time": "2024-11-10T06:36:14.878451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = WebGame()\n",
    "\n",
    "for episode in range(10):  # Запуск 10 игр\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done: \n",
    "        obs, reward, truncated, done, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "        env.render()    \n",
    "    print(f'Total reward for episode {episode} is {total_reward}')\n",
    "    env.close()  "
   ],
   "id": "da832e3e78b3a3c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for episode 0 is 16\n",
      "Total reward for episode 1 is 14\n",
      "Total reward for episode 2 is 16\n",
      "Total reward for episode 3 is 14\n",
      "Total reward for episode 4 is 15\n",
      "Total reward for episode 5 is 17\n",
      "Total reward for episode 6 is 22\n",
      "Total reward for episode 7 is 12\n",
      "Total reward for episode 8 is 29\n",
      "Total reward for episode 9 is 17\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:34:10.602770Z",
     "start_time": "2024-11-10T09:34:10.539589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "\n",
    "# Define a custom callback to save model checkpoints and track performance\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    def __init__(self, save_path, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.save_path = save_path\n",
    "        self.best_mean_reward = -float('inf')\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % 1000 == 0:  # Save every 1000 steps\n",
    "            reward = self.locals.get('reward', None)  # Get reward from locals\n",
    "            if reward and reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = reward\n",
    "                self.model.save(self.save_path)\n",
    "                print(f\"Saving new best model with reward: {self.best_mean_reward}\")\n",
    "        return True\n",
    "\n",
    "# Create the environment\n",
    "env = WebGame()  # Ensure WebGame is defined\n",
    "\n",
    "# Ensure the tensorboard log directory exists\n",
    "tensorboard_log_dir = \"dqn_tensorboard/\"\n",
    "os.makedirs(tensorboard_log_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = DQN('CnnPolicy', env, verbose=1, buffer_size=100000, batch_size=32, learning_starts=1000, optimize_memory_usage=False, tensorboard_log=tensorboard_log_dir)\n",
    "\n",
    "# Define the path to save the model\n",
    "model_save_path = \"best_model_dqn\"\n",
    "\n",
    "# Create the callback instance\n",
    "callback = SaveOnBestTrainingRewardCallback(save_path=model_save_path)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000, callback=callback)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"final_model_dqn\")\n"
   ],
   "id": "5a98aefa272de32d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 792. MiB for an array with shape (100000, 1, 1, 83, 100) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[142], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(tensorboard_log_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Initialize the model\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mDQN\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCnnPolicy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_starts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimize_memory_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Define the path to save the model\u001B[39;00m\n\u001B[0;32m     33\u001B[0m model_save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_model_dqn\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:141\u001B[0m, in \u001B[0;36mDQN.__init__\u001B[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexploration_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _init_setup_model:\n\u001B[1;32m--> 141\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001B[0m, in \u001B[0;36mDQN._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_setup_model\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_aliases()\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;66;03m# Copy running stats, see GH issue #996\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:189\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    187\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must pass an environment when using `HerReplayBuffer`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    188\u001B[0m         replay_buffer_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\n\u001B[1;32m--> 189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer_class(\n\u001B[0;32m    190\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_size,\n\u001B[0;32m    191\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space,\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space,\n\u001B[0;32m    193\u001B[0m         device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice,\n\u001B[0;32m    194\u001B[0m         n_envs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_envs,\n\u001B[0;32m    195\u001B[0m         optimize_memory_usage\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize_memory_usage,\n\u001B[0;32m    196\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mreplay_buffer_kwargs,\n\u001B[0;32m    197\u001B[0m     )\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_class(\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space,\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space,\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_schedule,\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_kwargs,\n\u001B[0;32m    204\u001B[0m )\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:212\u001B[0m, in \u001B[0;36mReplayBuffer.__init__\u001B[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001B[0m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReplayBuffer does not support optimize_memory_usage = True \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand handle_timeout_termination = True simultaneously.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m     )\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize_memory_usage \u001B[38;5;241m=\u001B[39m optimize_memory_usage\n\u001B[1;32m--> 212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservations \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_envs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobs_shape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m optimize_memory_usage:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When optimizing memory, `observations` contains also the next observation\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_observations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_envs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_shape), dtype\u001B[38;5;241m=\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mdtype)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 792. MiB for an array with shape (100000, 1, 1, 83, 100) and data type uint8"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:28:34.756278Z",
     "start_time": "2024-11-09T21:28:24.769718Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "env = WebGame()\n",
    "env_checker.check_env(env)"
   ],
   "id": "5845e512188aad64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:35:14.238264Z",
     "start_time": "2024-11-10T09:35:14.225262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ],
   "id": "ff25d23affc4a99e",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:35:15.408683Z",
     "start_time": "2024-11-10T09:35:15.403634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHECKPOINT_DIR = 'train/'\n",
    "LOG_DIR = 'logs/'"
   ],
   "id": "c71e6ba607dd8a64",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T09:35:16.204528Z",
     "start_time": "2024-11-10T09:35:16.190887Z"
    }
   },
   "cell_type": "code",
   "source": "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)",
   "id": "61aab82c9359426a",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:55:48.395749Z",
     "start_time": "2024-11-09T21:55:48.393323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ],
   "id": "c08dd31a9c621a70",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:55:51.057856Z",
     "start_time": "2024-11-09T21:55:51.046886Z"
    }
   },
   "cell_type": "code",
   "source": "env = WebGame()",
   "id": "f929fa02af9e0e46",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T22:03:36.203667Z",
     "start_time": "2024-11-09T22:03:36.113532Z"
    }
   },
   "cell_type": "code",
   "source": "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=100000, batch_size=32, learning_starts=1000, optimize_memory_usage=False)\n",
   "id": "ae58e1b7c7710b57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T22:09:03.168276Z",
     "start_time": "2024-11-09T22:09:01.557216Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=100000, callback=callback)",
   "id": "d61783cc03c7fce7",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001B[0m, in \u001B[0;36mDQN.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfDQN,\n\u001B[0;32m    260\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    265\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    266\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfDQN:\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:314\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfOffPolicyAlgorithm,\n\u001B[0;32m    307\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    312\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    313\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfOffPolicyAlgorithm:\n\u001B[1;32m--> 314\u001B[0m     total_timesteps, callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_learn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m     callback\u001B[38;5;241m.\u001B[39mon_training_start(\u001B[38;5;28mlocals\u001B[39m(), \u001B[38;5;28mglobals\u001B[39m())\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must set the environment before calling learn()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:297\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm._setup_learn\u001B[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001B[0m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_noise \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    292\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mnum_envs \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_noise, VectorizedActionNoise)\n\u001B[0;32m    294\u001B[0m ):\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_noise \u001B[38;5;241m=\u001B[39m VectorizedActionNoise(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_noise, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mnum_envs)\n\u001B[1;32m--> 297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_learn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    303\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:431\u001B[0m, in \u001B[0;36mBaseAlgorithm._setup_learn\u001B[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001B[0m\n\u001B[0;32m    429\u001B[0m \u001B[38;5;66;03m# Configure logger's outputs if no logger was passed\u001B[39;00m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_custom_logger:\n\u001B[1;32m--> 431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfigure_logger\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;66;03m# Create eval callback if needed\u001B[39;00m\n\u001B[0;32m    434\u001B[0m callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_callback(callback, progress_bar)\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\utils.py:201\u001B[0m, in \u001B[0;36mconfigure_logger\u001B[1;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001B[0m\n\u001B[0;32m    198\u001B[0m save_path, format_strings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstdout\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensorboard_log \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m SummaryWriter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying to log data to tensorboard but tensorboard is not installed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensorboard_log \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m SummaryWriter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    204\u001B[0m     latest_run_id \u001B[38;5;241m=\u001B[39m get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001B[1;31mImportError\u001B[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T22:03:41.168843Z",
     "start_time": "2024-11-09T22:03:41.047654Z"
    }
   },
   "cell_type": "code",
   "source": "model.load('train/best_model_88000')",
   "id": "20d1727a9af34739",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 19.94GB > 4.90GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x173168900d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:30:47.267672Z",
     "start_time": "2024-11-10T06:30:45.664511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for episode in range(5): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        time.sleep(0.01)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ],
   "id": "f3f3e659c428ec81",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[81], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m total_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done: \n\u001B[1;32m----> 6\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     obs, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;28mint\u001B[39m(action))\n\u001B[0;32m      8\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:255\u001B[0m, in \u001B[0;36mDQN.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    253\u001B[0m         action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample())\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 255\u001B[0m     action, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action, state\n",
      "File \u001B[1;32mD:\\Users\\Legion\\anaconda3\\envs\\env_pt\\lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(observation) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m--> 357\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    360\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    361\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    362\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m     )\n\u001B[0;32m    365\u001B[0m obs_tensor, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_to_tensor(observation)\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[1;31mValueError\u001B[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:28:46.804371Z",
     "start_time": "2024-11-10T06:28:46.795337Z"
    }
   },
   "cell_type": "code",
   "source": "env.close()",
   "id": "bbf2ae71d03097f4",
   "outputs": [],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
